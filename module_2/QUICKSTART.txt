QUICK START GUIDE - MODULE 2 WEB SCRAPER
========================================

Prerequisites: Python 3.10+, Internet connection


STEP 1: INSTALL DEPENDENCIES
=============================
cd module_2
pip install -r requirements.txt

Takes 1-2 minutes. Installs:
  • beautifulsoup4 (HTML parsing)
  • lxml (fast parser backend)


STEP 2: RUN THE SCRAPER
=======================
python scrape.py

What it does:
  • Connects to Grad Cafe
  • Scrapes pages 1-1500+ (~30,000+ entries)
  • Extracts: program, university, degree, GPA, GRE scores, status, etc.
  • Saves to applicant_data.json
  
Timeline:
  • Test run (100 pages): ~5-10 minutes
  • Full run (1500+ pages): ~30-60 minutes
  
Progress:
  • Shows updates every 10 pages
  • Shows total entries scraped


STEP 3: CLEAN THE DATA
======================
python clean.py

What it does:
  • Reads applicant_data.json
  • Standardizes formats:
    - GPA: "3.95"
    - GRE: "160"
    - Dates: "2023-12-15"
    - Status: "Accepted" / "Rejected" / "Waitlisted"
  • Separates program and university
  • Removes HTML remnants
  • Saves to applicant_data_cleaned.json
  
Timeline: ~1-2 minutes


STEP 4: LLM-BASED NAME STANDARDIZATION (OPTIONAL)
==================================================
cd llm_hosting
pip install -r requirements.txt
python app.py --file "../applicant_data_cleaned.json" > output.json

What it does:
  • Uses local LLM model to standardize names
  • Maps: "JHU", "Johns Hopkins", "John Hopkins" → "Johns Hopkins University"
  • Adds cleaned_program and cleaned_university columns
  • Generates final cleaned dataset
  
Timeline: ~5-10 minutes (depending on LLM model)


OUTPUT FILES CREATED
====================
✓ applicant_data.json
  - Raw scraped data from Grad Cafe
  - 30,000-40,000 entries
  - All fields extracted as-is
  
✓ applicant_data_cleaned.json
  - Standardized formats
  - Consistent missing data (None values)
  - Separated program and university fields
  
✓ output.json (after LLM phase)
  - LLM-standardized program and university names
  - Ready for analysis
  - Full traceability with original_program field


TROUBLESHOOTING
===============

Q: "ModuleNotFoundError: No module named 'bs4'"
A: pip install -r requirements.txt

Q: "urllib.error.URLError: urlopen error..."
A: Check internet connection. Grad Cafe may be down. Try again later.

Q: "HTTP Error 404 on page X"
A: Normal - means you've reached the end of results. Scraper will stop.

Q: "Scraper running very slowly"
A: Normal for large datasets. Full 30k+ entries takes 30-60 minutes.
   You can reduce pages parameter in main() to test faster.

Q: "Memory error with large dataset"
A: If processing all 30,000 entries at once causes issues:
   - Run on machine with more RAM
   - Or process in smaller batches


CUSTOMIZATION
==============

To scrape fewer entries for testing:
  Edit scrape.py, line in main():
    data = scrape_data(result_type='all', num_pages=100)  # Change 100
  
To scrape specific types:
  data = scrape_data(result_type='accepted', num_pages=500)
  Options: 'all', 'accepted', 'rejected', 'waitlisted'

To adjust timeout:
  Edit scrape.py, line ~80:
    with urlopen(request, timeout=10) as response:  # Change 10


VERIFICATION
============

Check that files are present:
  ls  or dir
  
Expected files after completion:
  ✓ scrape.py
  ✓ clean.py
  ✓ requirements.txt
  ✓ README.txt
  ✓ applicant_data.json (30MB+)
  ✓ applicant_data_cleaned.json


DATA INSPECTION
================

View first few entries:
  python -c "import json; data=json.load(open('applicant_data.json')); print(json.dumps(data[0], indent=2))"

Count entries:
  python -c "import json; data=json.load(open('applicant_data.json')); print(f'Total entries: {len(data)}')"

View specific fields:
  python -c "import json; data=json.load(open('applicant_data.json')); print([d.get('program') for d in data[:5]])"


NEXT: GITHUB SUBMISSION
=======================

1. Initialize git (if not already done):
   git init
   git add .
   git commit -m "Module 2: Web scraper for Grad Cafe data"

2. Create private repo on GitHub:
   Name: jhu_software_concepts
   Folder: module_2

3. Push to GitHub:
   git remote add origin https://github.com/[username]/jhu_software_concepts.git
   git branch -M main
   git push -u origin main

4. Verify on GitHub:
   Check: jhu_software_concepts/module_2/

5. Submit on Canvas:
   - Download module_2 as .zip
   - Note the timestamp
   - Upload to Canvas
   - Keep timestamp for proof of on-time submission


---
Setup time: ~5-10 minutes
Scraping time: 30-60 minutes (full dataset)
Total project time: 1-2 hours end-to-end
